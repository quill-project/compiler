
mod quill::lexer

use quill::*
use quill::token::(Token, TokenType)

// Returns if a character is alphabetic, meaning in the range of 'a'..'z' 
// OR 'A'..'Z'.
pub fun is_alphabetic(char: String) -> Bool {
    val c = char |> as_code()
    return (65 <= c && c <= 90)
        || (97 <= c && c <= 122)
}

// Returns if a character is numeric, meaning in the range of '0'..'9'.
pub fun is_numeric(char: String) -> Bool {
    val c = char |> as_code()
    return (48 <= c && c <= 57)
}

// Returns if a character is alphabetic (according to 'is_alphabetic'),
// numeric (according to 'is_numeric') or equal to '_'.
pub fun is_alphanumeric(char: String) -> Bool = char == "_"
    || is_alphabetic(char) 
    || is_numeric(char)

// Returns if the character is an ASCII whitespace character, these being
// space, new line, carriage return, horizontal tab and vertical tab.
pub fun is_whitespace(char: String) -> Bool = char == " "
    || char == "\n" // new line
    || char == "\r" // carriage return
    || char == "\x09" // horizontal tab
    || char == "\x0B" // vertical tab

// Attempts to parse the hexadecimal digit given by the first character 
// of the given string into an integer.
pub fun parse_hex_digit(char: String) -> Option[Int] {
    val c = char |> as_code()
    return if 48 <= c && c <= 57 { Option::Some(c - 48) } // '0' - '9'
        else if 65 <= c && c <= 70 { Option::Some(c - 65 + 10) } // 'A' - 'F'
        else if 97 <= c && c <= 102 { Option::Some(c - 97 + 10) } // 'a' - 'f'
        else { Option::None[Int] }
}

// Attempts to parse an entire hexadecimal string into an integer.
pub fun parse_hex_number(number: String) -> Option[Int] = number 
    |> chars() 
    |> fold(Option::Some(0), |acc, next| acc |> and_then(|prev| next
        |> parse_hex_digit()
        |> map(|d| prev * 16 + d)
    ))

// Maps Quill keywords to their respective keyword token types.
pub val KEYWORDS: Map[String, TokenType] = Map::of(
    Pair("if", TokenType::KeywordIf),
    Pair("else", TokenType::KeywordElse),
    Pair("ext", TokenType::KeywordExt),
    Pair("fun", TokenType::KeywordFun),
    Pair("return", TokenType::KeywordReturn),
    Pair("continue", TokenType::KeywordContinue),
    Pair("break", TokenType::KeywordBreak),
    Pair("val", TokenType::KeywordVal),
    Pair("mut", TokenType::KeywordMut),
    Pair("mod", TokenType::KeywordMod),
    Pair("use", TokenType::KeywordUse),
    Pair("as", TokenType::KeywordAs),
    Pair("pub", TokenType::KeywordPub),
    Pair("struct", TokenType::KeywordStruct),
    Pair("enum", TokenType::KeywordEnum),
    Pair("match", TokenType::KeywordMatch),
    Pair("while", TokenType::KeywordWhile),
    Pair("for", TokenType::KeywordFor),

    Pair("true", TokenType::BoolLiteral),
    Pair("false", TokenType::BoolLiteral),
    Pair("unit", TokenType::UnitLiteral)
)

// Turns a given source file into a stream of tokens, including tokens
// for whitespaces, comments and invalid characters.
// The parser expects these to be filtered out, as is the case with the output
// returned by 'tokenize'.
pub fun tokenize_all(
    path: String, content: String, messages: mut List[Message]
) -> mut Stream[Token] {
    val content_chars = content |> chars()
    val current = Pair("", "")
    mut offset = -2
    mut collected = ""
    val advance: Fun() = || {
        collected = collected |> concat(current.first)
        current.first = current.second
        current.second = content_chars |> next() |> unwrap_or("")
        offset = offset + 1
    }
    advance()
    advance()
    val advance_while: Fun(Fun(String) -> Bool) 
        = |cond| Stream::new[String](|| current.first) 
        |> take_while(|c| c != "" && cond(c))
        |> for_each(|_| advance())
    return Stream::new(|| {
        val start = offset
        collected = ""
        val build_token: Fun(TokenType) -> Token 
            = |tt| Token(tt, collected, Source(path, start, offset))
        val coll_token: Fun(TokenType, Int) -> Token 
            = |tt, l| {
                Stream::new[Unit](|| unit) |> take(l) |> for_each(|_| advance())
                return Token(tt, collected, Source(path, start, offset))
            } 
        if current.first == "" { 
            return build_token(TokenType::EndOfFile)
        }
        if is_whitespace(current.first) {
            advance_while(is_whitespace)
            return build_token(TokenType::Whitespace)
        }
        match current {
            Pair("/", "/") {
                advance_while(|c| c != "\n")
                advance()
                return build_token(TokenType::LineComment)
            }
            Pair(".", ".") {
                advance()
                if current.second != "." {
                    return build_token(TokenType::Dot)
                }
                advance()
                advance()
                return build_token(TokenType::TripleDots)
            }
            Pair("<", "=") { return coll_token(TokenType::LessThanEqual, 2) }
            Pair(">", "=") { return coll_token(TokenType::GreaterThanEqual, 2) }
            Pair("=", "=") { return coll_token(TokenType::DoubleEqual, 2) }
            Pair("!", "=") { return coll_token(TokenType::NotEqual, 2) }
            Pair("-", ">") { return coll_token(TokenType::ArrowRight, 2) }
            Pair("&", "&") { return coll_token(TokenType::DoubleAmpersand, 2) }
            Pair("|", "|") { return coll_token(TokenType::DoublePipe, 2) }
            Pair(":", ":") { return coll_token(TokenType::PathSeparator, 2) }
            Pair("|", ">") { return coll_token(TokenType::Triangle, 2) }
            Pair("(", _) { return coll_token(TokenType::ParenOpen, 1) }
            Pair(")", _) { return coll_token(TokenType::ParenClose, 1) }
            Pair("{", _) { return coll_token(TokenType::BraceOpen, 1) }
            Pair("}", _) { return coll_token(TokenType::BraceClose, 1) }
            Pair("[", _) { return coll_token(TokenType::BracketOpen, 1) }
            Pair("]", _) { return coll_token(TokenType::BracketClose, 1) }
            Pair("<", _) { return coll_token(TokenType::LessThan, 1) }
            Pair(">", _) { return coll_token(TokenType::GreaterThan, 1) }
            Pair("=", _) { return coll_token(TokenType::Equal, 1) }
            Pair("+", _) { return coll_token(TokenType::Plus, 1) }
            Pair("-", _) { return coll_token(TokenType::Minus, 1) }
            Pair("*", _) { return coll_token(TokenType::Asterisk, 1) }
            Pair("/", _) { return coll_token(TokenType::Slash, 1) }
            Pair("%", _) { return coll_token(TokenType::Percent, 1) }
            Pair(":", _) { return coll_token(TokenType::Colon, 1) }
            Pair(",", _) { return coll_token(TokenType::Comma, 1) }
            Pair("!", _) { return coll_token(TokenType::ExclamationMark, 1) }
            Pair(".", _) { return coll_token(TokenType::Dot, 1) }
            Pair("|", _) { return coll_token(TokenType::Pipe, 1) }
            Pair("@", _) { return coll_token(TokenType::At, 1) }
            _ {}
        }
        if is_numeric(current.first) {
            advance_while(is_numeric)
            mut tt = TokenType::IntLiteral
            if current.first == "." {
                advance()
                advance_while(is_numeric)
                tt = TokenType::FloatLiteral
            }
            return build_token(tt)
        }
        if is_alphanumeric(current.first) {
            advance_while(is_alphanumeric)
            return KEYWORDS |> get(collected) 
                |> unwrap_or(TokenType::Identifier) 
                |> build_token()
        }
        if current.first == "\"" {
            advance()
            val content = expanded_string_escapes(
                || current, advance, path, || offset, messages
            ) |> join("")
            advance()
            return Token(
                TokenType::StringLiteral, content, Source(path, start, offset)
            )
        }
        messages |> push(Message::invalid_character(
            current.first, Source(path, offset, offset + 1)
        ))
        advance()
        return build_token(TokenType::Invalid)
    })
}

fun expanded_string_escapes(
    current: Fun() -> Pair[String, String], advance: Fun(), 
    path: String, offset: Fun() -> Int, messages: mut List[Message]
) -> mut Sequence[String] {
    val start = offset()
    val expanded: Fun(String) -> Option[String] = |r| {
        advance()
        advance()
        return Option::Some(r)
    }
    return Sequence::new[String](|| {
        match current() {
            Pair("\\", "\"") { return expanded("\"") }
            Pair("\\", "\n") { return expanded("") }
            Pair("\\", "b") { return expanded("\x08") } // backspace
            Pair("\\", "f") { return expanded("\x0C") } // formfeed
            Pair("\\", "n") { return expanded("\n") }
            Pair("\\", "r") { return expanded("\r") } // carriage return
            Pair("\\", "t") { return expanded("\x09") } // horizontal tab
            Pair("\\", "x") {
                advance()
                advance()
                val c = current().first |> concat(current().second) 
                    |> parse_hex_number()
                    |> map(String::from_code)
                    |> unwrap_or_else(|| {
                        val s = Source(path, offset(), offset() + 2)
                        messages |> push(Message::invalid_hex_escape(s))
                        return ""
                    })
                advance()
                advance()
                return Option::Some(c)
            } 
            Pair("\\", "u") {
                advance()
                advance()
                val fh = current().first |> concat(current().second)
                advance()
                advance()
                val c = fh 
                    |> concat(current().first) |> concat(current().second)
                    |> parse_hex_number()
                    |> map(String::from_code)
                    |> unwrap_or_else(|| {
                        val s = Source(path, offset(), offset() + 4)
                        messages |> push(Message::invalid_hex_escape(s))
                        return ""
                    })
                advance()
                advance()
                return Option::Some(c)
            } 
            Pair("\\", c) { return expanded(c) }
            Pair("\"", _) { return Option::None }
            Pair("", _) {
                val s = Source(path, start, offset() + 1)
                messages |> push(Message::unclosed_string_literal(s))
                return Option::None
            }
            Pair(c, _) {
                advance()
                return Option::Some(c)
            }
        }
    })
}

// Turns a given source file into a stream of tokens, excluding tokens
// for whitespaces, comments and invalid characters, as is expected by the parser.
// Tokens are filtered according to 'TokenType::is_relevant'.
// To include these tokens in the output, use 'tokenize_all' instead.
pub fun tokenize(
    path: String, content: String, messages: mut List[Message]
) -> mut Stream[Token]
    = tokenize_all(path, content, messages) 
    |> filter(|t| t.type |> is_relevant())